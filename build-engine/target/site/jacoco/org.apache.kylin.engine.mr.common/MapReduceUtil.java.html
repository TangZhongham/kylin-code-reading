<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="zh"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>MapReduceUtil.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Apache Kylin - Build Engine</a> &gt; <a href="index.source.html" class="el_package">org.apache.kylin.engine.mr.common</a> &gt; <span class="el_source">MapReduceUtil.java</span></div><h1>MapReduceUtil.java</h1><pre class="source lang-java linenums">/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *  
 *     http://www.apache.org/licenses/LICENSE-2.0
 *  
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.kylin.engine.mr.common;

import java.io.IOException;
import java.util.Map;
import java.util.Set;

import org.apache.hadoop.mapreduce.Reducer;
import org.apache.kylin.common.KylinConfig;
import org.apache.kylin.common.util.Pair;
import org.apache.kylin.cube.CubeInstance;
import org.apache.kylin.cube.CubeSegment;
import org.apache.kylin.cube.cuboid.CuboidScheduler;
import org.apache.kylin.cube.model.CubeDesc;
import org.apache.kylin.job.exception.JobException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.collect.Sets;

<span class="nc" id="L38">public class MapReduceUtil {</span>

<span class="nc" id="L40">    private static final Logger logger = LoggerFactory.getLogger(MapReduceUtil.class);</span>

    /**
     * @return reducer number for calculating hll
     */
    public static int getCuboidHLLCounterReducerNum(CubeInstance cube) {
<span class="nc" id="L46">        int nCuboids = cube.getCuboidScheduler().getAllCuboidIds().size();</span>
<span class="nc" id="L47">        int shardBase = (nCuboids - 1) / cube.getConfig().getHadoopJobPerReducerHLLCuboidNumber() + 1;</span>

<span class="nc" id="L49">        int hllMaxReducerNumber = cube.getConfig().getHadoopJobHLLMaxReducerNumber();</span>
<span class="nc bnc" id="L50" title="All 2 branches missed.">        if (shardBase &gt; hllMaxReducerNumber) {</span>
<span class="nc" id="L51">            shardBase = hllMaxReducerNumber;</span>
        }
<span class="nc" id="L53">        return shardBase;</span>
    }

    /**
     * @param cuboidScheduler specified can provide more flexibility
     * */
    public static int getLayeredCubingReduceTaskNum(CubeSegment cubeSegment, CuboidScheduler cuboidScheduler,
            double totalMapInputMB, int level)
            throws ClassNotFoundException, IOException, InterruptedException, JobException {
<span class="nc" id="L62">        CubeDesc cubeDesc = cubeSegment.getCubeDesc();</span>
<span class="nc" id="L63">        KylinConfig kylinConfig = cubeDesc.getConfig();</span>

<span class="nc" id="L65">        double perReduceInputMB = kylinConfig.getDefaultHadoopJobReducerInputMB();</span>
<span class="nc" id="L66">        double reduceCountRatio = kylinConfig.getDefaultHadoopJobReducerCountRatio();</span>
<span class="nc" id="L67">        logger.info(&quot;Having per reduce MB &quot; + perReduceInputMB + &quot;, reduce count ratio &quot; + reduceCountRatio + &quot;, level &quot;</span>
                + level);

<span class="nc" id="L70">        CubeStatsReader cubeStatsReader = new CubeStatsReader(cubeSegment, cuboidScheduler, kylinConfig);</span>

        double parentLayerSizeEst, currentLayerSizeEst, adjustedCurrentLayerSizeEst;

<span class="nc bnc" id="L74" title="All 2 branches missed.">        if (level == -1) {</span>
            //merge case
<span class="nc" id="L76">            double estimatedSize = cubeStatsReader.estimateCubeSize();</span>
<span class="nc bnc" id="L77" title="All 2 branches missed.">            adjustedCurrentLayerSizeEst = estimatedSize &gt; totalMapInputMB ? totalMapInputMB : estimatedSize;</span>
<span class="nc" id="L78">            logger.debug(&quot;estimated size {}, input size {}, adjustedCurrentLayerSizeEst: {}&quot;, estimatedSize,</span>
<span class="nc" id="L79">                    totalMapInputMB, adjustedCurrentLayerSizeEst);</span>
<span class="nc bnc" id="L80" title="All 2 branches missed.">        } else if (level == 0) {</span>
            //base cuboid case TODO: the estimation could be very WRONG because it has no correction
<span class="nc" id="L82">            adjustedCurrentLayerSizeEst = cubeStatsReader.estimateLayerSize(0);</span>
<span class="nc" id="L83">            logger.debug(&quot;adjustedCurrentLayerSizeEst: {}&quot;, adjustedCurrentLayerSizeEst);</span>
        } else {
<span class="nc" id="L85">            parentLayerSizeEst = cubeStatsReader.estimateLayerSize(level - 1);</span>
<span class="nc" id="L86">            currentLayerSizeEst = cubeStatsReader.estimateLayerSize(level);</span>
<span class="nc" id="L87">            adjustedCurrentLayerSizeEst = totalMapInputMB / parentLayerSizeEst * currentLayerSizeEst;</span>
<span class="nc" id="L88">            logger.debug(</span>
                    &quot;totalMapInputMB: {}, parentLayerSizeEst: {}, currentLayerSizeEst: {}, adjustedCurrentLayerSizeEst: {}&quot;,
<span class="nc" id="L90">                    totalMapInputMB, parentLayerSizeEst, currentLayerSizeEst, adjustedCurrentLayerSizeEst);</span>
        }

        // number of reduce tasks
<span class="nc" id="L94">        int numReduceTasks = (int) Math.round(adjustedCurrentLayerSizeEst / perReduceInputMB * reduceCountRatio + 0.99);</span>

        // adjust reducer number for cube which has DISTINCT_COUNT measures for better performance
<span class="nc bnc" id="L97" title="All 2 branches missed.">        if (cubeDesc.hasMemoryHungryMeasures()) {</span>
<span class="nc" id="L98">            logger.debug(&quot;Multiply reducer num by 4 to boost performance for memory hungry measures&quot;);</span>
<span class="nc" id="L99">            numReduceTasks = numReduceTasks * 4;</span>
        }

        // at least 1 reducer by default
<span class="nc" id="L103">        numReduceTasks = Math.max(kylinConfig.getHadoopJobMinReducerNumber(), numReduceTasks);</span>
        // no more than 500 reducer by default
<span class="nc" id="L105">        numReduceTasks = Math.min(kylinConfig.getHadoopJobMaxReducerNumber(), numReduceTasks);</span>

<span class="nc" id="L107">        return numReduceTasks;</span>
    }

    public static int getInmemCubingReduceTaskNum(CubeSegment cubeSeg, CuboidScheduler cuboidScheduler)
            throws IOException {
<span class="nc" id="L112">        KylinConfig kylinConfig = cubeSeg.getConfig();</span>

<span class="nc" id="L114">        Map&lt;Long, Double&gt; cubeSizeMap = new CubeStatsReader(cubeSeg, cuboidScheduler, kylinConfig).getCuboidSizeMap();</span>
<span class="nc" id="L115">        double totalSizeInM = 0;</span>
<span class="nc bnc" id="L116" title="All 2 branches missed.">        for (Double cuboidSize : cubeSizeMap.values()) {</span>
<span class="nc" id="L117">            totalSizeInM += cuboidSize;</span>
<span class="nc" id="L118">        }</span>
<span class="nc" id="L119">        return getReduceTaskNum(totalSizeInM, kylinConfig);</span>
    }

    // @return the first indicates the total reducer number, the second indicates the reducer number for base cuboid
    public static Pair&lt;Integer, Integer&gt; getConvergeCuboidDataReduceTaskNums(CubeSegment cubeSeg) throws IOException {
<span class="nc" id="L124">        long baseCuboidId = cubeSeg.getCuboidScheduler().getBaseCuboidId();</span>

<span class="nc" id="L126">        Set&lt;Long&gt; overlapCuboids = Sets.newHashSet(cubeSeg.getCuboidScheduler().getAllCuboidIds());</span>
<span class="nc" id="L127">        overlapCuboids.retainAll(cubeSeg.getCubeInstance().getCuboidsRecommend());</span>
<span class="nc" id="L128">        overlapCuboids.add(baseCuboidId);</span>

<span class="nc" id="L130">        Pair&lt;Map&lt;Long, Long&gt;, Long&gt; cuboidStats = CuboidStatsReaderUtil</span>
<span class="nc" id="L131">                .readCuboidStatsWithSourceFromSegment(overlapCuboids, cubeSeg);</span>
<span class="nc" id="L132">        Map&lt;Long, Double&gt; cubeSizeMap = CubeStatsReader.getCuboidSizeMapFromRowCount(cubeSeg, cuboidStats.getFirst(),</span>
<span class="nc" id="L133">                cuboidStats.getSecond());</span>
<span class="nc" id="L134">        double totalSizeInM = 0;</span>
<span class="nc bnc" id="L135" title="All 2 branches missed.">        for (Double cuboidSize : cubeSizeMap.values()) {</span>
<span class="nc" id="L136">            totalSizeInM += cuboidSize;</span>
<span class="nc" id="L137">        }</span>

<span class="nc" id="L139">        double baseSizeInM = cubeSizeMap.get(baseCuboidId);</span>

<span class="nc" id="L141">        KylinConfig kylinConfig = cubeSeg.getConfig();</span>
<span class="nc" id="L142">        int nBase = getReduceTaskNum(baseSizeInM, kylinConfig);</span>
<span class="nc" id="L143">        int nOther = getReduceTaskNum(totalSizeInM - baseSizeInM, kylinConfig);</span>
<span class="nc" id="L144">        return new Pair&lt;&gt;(nBase + nOther, nBase);</span>
    }

    private static int getReduceTaskNum(double totalSizeInM, KylinConfig kylinConfig) {
<span class="nc" id="L148">        double perReduceInputMB = kylinConfig.getDefaultHadoopJobReducerInputMB();</span>
<span class="nc" id="L149">        double reduceCountRatio = kylinConfig.getDefaultHadoopJobReducerCountRatio();</span>

        // number of reduce tasks
<span class="nc" id="L152">        int numReduceTasks = (int) Math.round(totalSizeInM / perReduceInputMB * reduceCountRatio);</span>

        // at least 1 reducer by default
<span class="nc" id="L155">        numReduceTasks = Math.max(kylinConfig.getHadoopJobMinReducerNumber(), numReduceTasks);</span>
        // no more than 500 reducer by default
<span class="nc" id="L157">        numReduceTasks = Math.min(kylinConfig.getHadoopJobMaxReducerNumber(), numReduceTasks);</span>

<span class="nc" id="L159">        logger.info(&quot;Having total map input MB &quot; + Math.round(totalSizeInM));</span>
<span class="nc" id="L160">        logger.info(&quot;Having per reduce MB &quot; + perReduceInputMB);</span>
<span class="nc" id="L161">        logger.info(&quot;Setting &quot; + Reducer.Context.NUM_REDUCES + &quot;=&quot; + numReduceTasks);</span>
<span class="nc" id="L162">        return numReduceTasks;</span>
    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.2.201808211720</span></div></body></html>